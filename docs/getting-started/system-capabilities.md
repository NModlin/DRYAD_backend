# ğŸ¤– DRYAD.AI Backend - System Capabilities & Connections

## Overview

DRYAD.AI is a **self-contained AI backend** designed for local operation with optional external service integrations.

---

## ğŸ§  Core AI Capabilities

### 1. Local LLM Processing
**Primary Provider: LlamaCpp**
- âœ… **Status**: Active (TinyLlama 1.1B Q4_K_M)
- ğŸ“¦ **Dependency**: `llama-cpp-python>=0.2.0`
- ğŸ¯ **Purpose**: Self-contained AI without external API dependencies
- ğŸš€ **Features**:
  - GGUF model format support
  - CPU/GPU inference (auto-detection)
  - Memory-mapped model loading
  - Optimized thread management
  - Context window: 2048-8192 tokens (dynamic)

**Alternative Providers**:
- **Ollama**: Local LLM server (http://localhost:11434)
  - Models: llama3.2:3b, llama3.2:1b, tinyllama
- **Hugging Face**: Transformers-based models
  - Model: microsoft/DialoGPT-medium
- **Mock**: Testing/development fallback

### 2. Multi-Agent Orchestration
- âœ… **Built-in system** (no CrewAI dependency)
- ğŸ¤– **Available Agents**:
  - Research Agent
  - Analysis Agent
  - Writing Agent
  - Code Agent
  - Planning Agent
- ğŸ”„ **Workflows**:
  - Simple research
  - Research â†’ Analyze â†’ Write
  - Complex analysis
  - Content creation
- âš¡ **Advantages**:
  - No external dependencies
  - Faster execution
  - Better error handling
  - Lower memory footprint

### 3. Vector Search & RAG
**Provider: Weaviate**
- ğŸ”— **Connection**: http://localhost:8080
- ğŸ“Š **Embedding Model**: all-MiniLM-L6-v2 (384 dimensions)
- ğŸ¨ **CLIP Support**: Multi-modal embeddings (optional)
- ğŸ” **Search Types**:
  - Semantic vector search
  - BM25 keyword search
  - Hybrid search
  - Document similarity
- ğŸ“ **Collection**: GremlinsDocument

### 4. Multi-Modal Processing
- ğŸµ **Audio**: Speech-to-text (Whisper)
- ğŸ–¼ï¸ **Images**: CLIP embeddings, OCR
- ğŸ¥ **Video**: Frame extraction, analysis
- ğŸ“„ **Documents**: PDF, DOCX, TXT parsing
- ğŸ’¾ **Storage**: Local filesystem (`./data/multimodal`)

---

## ğŸ—„ï¸ Data Storage

### 1. Primary Database
**SQLite**
- ğŸ“ **Location**: `sqlite:///./data/DRYAD.AI.db`
- ğŸ”§ **ORM**: SQLAlchemy 2.0+ (async)
- ğŸ”„ **Migrations**: Alembic
- ğŸ“Š **Data Types**:
  - Users & authentication
  - Documents & metadata
  - Chat history
  - Organizations
  - Client applications
  - Shared knowledge base

### 2. Vector Database
**Weaviate**
- ğŸ“ **URL**: http://localhost:8080
- ğŸ¯ **Purpose**: Semantic search, RAG
- ğŸ“¦ **Status**: Optional (graceful degradation)

### 3. Cache & Task Queue
**Redis**
- ğŸ“ **URL**: redis://localhost:6379
- ğŸ¯ **Purpose**: 
  - Response caching
  - Session storage
  - Task queue (Celery)
- ğŸ“¦ **Status**: Optional

---

## ğŸ” Authentication & Security

### 1. OAuth 2.0
**Google OAuth**
- âœ… **Status**: Fully configured
- ğŸ”‘ **Client ID**: 818968828866-...
- ğŸŒ **Redirect**: http://localhost:3000/auth/callback
- ğŸ« **Scopes**: openid, email, profile

### 2. JWT Tokens
- ğŸ” **Algorithm**: HS256
- â±ï¸ **Access Token**: 1 hour
- ğŸ”„ **Refresh Token**: 30 days (HttpOnly cookie)
- ğŸ›¡ï¸ **Security**: Client IP + User Agent tracking

### 3. Advanced Security
- ğŸ”‘ **API Key Management**: Rate limiting, expiration
- ğŸ” **Data Encryption**: Fernet (AES-128)
- ğŸ•µï¸ **PII Detection**: Automatic redaction
- ğŸ“ **Audit Logging**: Security events
- ğŸš¨ **Threat Detection**: Risk scoring

---

## ğŸŒ API Interfaces

### 1. REST API
- ğŸ“ **Base URL**: http://localhost:8000
- ğŸ“š **Docs**: http://localhost:8000/docs
- ğŸ”¢ **Endpoints**: 35+
- ğŸ“‹ **Categories**:
  - `/api/v1/auth` - Authentication
  - `/api/v1/chat` - Chat & conversations
  - `/api/v1/documents` - Document management
  - `/api/v1/orchestrator` - Multi-agent tasks
  - `/api/v1/multimodal` - Multi-modal processing
  - `/api/v1/health` - System health
  - `/api/v1/mcp` - Model Context Protocol

### 2. WebSocket API
- ğŸ“ **URL**: ws://localhost:8000/api/v1/ws/ws
- ğŸ”„ **Real-time**: Bidirectional communication
- ğŸ“¨ **Message Types**: 13+
- ğŸ¯ **Use Cases**:
  - Live chat streaming
  - Agent workflow updates
  - System notifications
  - Progress tracking

### 3. GraphQL API
- ğŸ“ **URL**: http://localhost:8000/graphql
- ğŸ“Š **Types**: 12+
- ğŸ¯ **Features**:
  - Flexible queries
  - Nested data fetching
  - Real-time subscriptions

### 4. MCP (Model Context Protocol)
- ğŸ“ **URL**: http://localhost:8000/api/v1/mcp
- ğŸ¯ **Purpose**: Standardized AI model communication
- ğŸ”Œ **Capabilities**: Dynamic capability discovery

---

## ğŸ”— External Service Integrations

### Currently Connected

1. **Google OAuth** âœ…
   - Authentication provider
   - User profile data

2. **Google Gemini API** âš ï¸ (Optional)
   - API Key: Configured
   - Model: gemini-1.5-pro
   - Purpose: Project proposal generation
   - Status: Optional feature

### Available (Not Connected)

3. **Weaviate** âš ï¸
   - Status: Degraded (not running)
   - Fallback: In-memory search

4. **Redis** âš ï¸
   - Status: Not connected
   - Fallback: In-memory cache

5. **Ollama** âš ï¸
   - Status: Not running
   - Alternative: LlamaCpp (active)

### Optional Integrations

6. **MinIO** (Object Storage)
   - Endpoint: localhost:9000
   - Status: Not configured

7. **Kafka** (Event Streaming)
   - Broker: localhost:9092
   - Status: Not configured

8. **Qdrant** (Alternative Vector DB)
   - Host: localhost:6333
   - Status: Not configured

---

## ğŸ“Š Monitoring & Observability

### 1. Metrics Collection
- ğŸ“ˆ **System Metrics**: CPU, memory, disk
- âš¡ **Performance**: Request latency, throughput
- ğŸ¤– **AI Metrics**: LLM latency, token usage
- ğŸ”„ **Agent Metrics**: Workflow success rate

### 2. Health Monitoring
- ğŸ¥ **Health Endpoint**: `/api/v1/health/status`
- ğŸ” **Checks**:
  - Database connectivity
  - LLM availability
  - Vector store status
  - Service dependencies

### 3. Logging
- ğŸ“ **Structured JSON**: All logs
- ğŸ“ **Files**:
  - `logs/gremlins_app.log` - All logs
  - `logs/gremlins_errors.log` - Errors only
  - `logs/gremlins_access.log` - API access
- ğŸšï¸ **Level**: WARNING (silent mode)

### 4. Distributed Tracing
- ğŸ”— **Request Tracking**: Unique request IDs
- ğŸ“Š **Performance**: End-to-end latency
- ğŸ› **Debugging**: Error propagation

---

## ğŸš€ Deployment Options

### 1. Basic Mode (Current)
```bash
python start.py basic
```
- âœ… SQLite database
- âœ… Local LLM (LlamaCpp)
- âœ… In-memory cache
- âœ… No external services required

### 2. Development Mode
```bash
python start.py development
```
- âœ… All basic features
- âœ… Hot reload
- âœ… Debug logging
- âœ… Development tools

### 3. Docker Basic
```bash
python start.py docker-basic
```
- ğŸ³ Containerized backend
- ğŸ³ Redis container
- ğŸ³ Weaviate container

### 4. Docker Full
```bash
python start.py docker-full
```
- ğŸ³ All services containerized
- ğŸ³ Nginx reverse proxy
- ğŸ³ Ollama LLM server
- ğŸ³ Celery workers
- ğŸ³ Production-ready

---

## ğŸ¯ Self-Healing Capability (Proposed)

### Can the backend watch its own logs and fix errors?

**YES!** This is absolutely possible. Here's what we can build:

### Architecture

1. **Log Monitor** ğŸ‘€
   - Watch `logs/gremlins_errors.log` in real-time
   - Detect error patterns
   - Classify error severity

2. **Error Analyzer** ğŸ”
   - Parse stack traces
   - Identify root cause
   - Find affected code files

3. **Code Generator** ğŸ¤–
   - Use local LLM to generate fixes
   - Analyze codebase context
   - Create patches

4. **Test & Validate** âœ…
   - Run unit tests
   - Verify fix works
   - Check for regressions

5. **Auto-Apply** ğŸ”§
   - Apply code changes
   - Restart affected services
   - Monitor for new errors

6. **Rollback** â†©ï¸
   - Undo if fix fails
   - Restore previous version
   - Alert human operator

### Implementation Plan

Would you like me to implement this self-healing system? It would include:

- Real-time log monitoring
- AI-powered error analysis
- Automatic code generation
- Safe patch application
- Rollback mechanism
- Human approval workflow (optional)

---

## ğŸ“¦ Dependencies Summary

### Core (Required)
- FastAPI, Uvicorn
- SQLAlchemy, Alembic
- Pydantic
- llama-cpp-python
- sentence-transformers

### Optional (Graceful Degradation)
- weaviate-client
- redis
- celery
- whisper (audio)
- PIL, opencv (images)

### External APIs (Optional)
- Google OAuth (authentication)
- Google Gemini (proposals)

---

## ğŸ›ï¸ Configuration

All settings in `.env`:
- LLM provider & model
- Database URL
- External service URLs
- API keys
- Feature flags
- Security settings

---

## ğŸ“ˆ Current Status

âœ… **Fully Operational**:
- Local LLM (LlamaCpp)
- SQLite database
- REST API
- WebSocket API
- OAuth authentication
- Multi-agent system
- Silent logging

âš ï¸ **Degraded** (Optional):
- Weaviate (not running)
- Redis (not connected)
- Ollama (not running)

ğŸ”§ **Ready to Implement**:
- Self-healing system
- Advanced monitoring
- Auto-scaling
- Distributed deployment

---

**Want me to implement the self-healing capability?** ğŸ¤–


