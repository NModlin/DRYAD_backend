# DRYAD.AI Backend - Docker Compose Configuration
# Supports different deployment scenarios

version: '3.8'

services:
  # DRYAD.AI Backend Application (Unified Installation)
  gremlins-api:
    build:
      context: .
      dockerfile: Dockerfile
      target: production  # Use unified production build
    container_name: gremlins-api
    ports:
      - "8000:8000"
    environment:
      # Database
      - DATABASE_URL=sqlite:///app/data/DRYAD.AI.db
      
      # LLM Configuration
      - LLM_PROVIDER=${LLM_PROVIDER:-llamacpp}
      - LLAMACPP_MODEL=${LLAMACPP_MODEL:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OLLAMA_BASE_URL=http://ollama:11434
      
      # Vector Database
      - WEAVIATE_URL=http://weaviate:8081
      - WEAVIATE_API_KEY=${WEAVIATE_API_KEY:-}
      
      # Task Queue
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      
      # Security
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:?JWT_SECRET_KEY is required - generate with: python -c 'import secrets; print(secrets.token_urlsafe(48))'}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID:-}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET:-}
      
      # Application
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      weaviate:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - gremlins-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Weaviate Vector Database
  weaviate:
    image: semitechnologies/weaviate:1.27.0
    container_name: gremlins-weaviate
    ports:
      - "8081:8080"
      - "50051:50051"  # gRPC port
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: 'text2vec-transformers,text2vec-openai,generative-openai'
      CLUSTER_HOSTNAME: 'node1'
      LOG_LEVEL: 'info'
    volumes:
      - weaviate_data:/var/lib/weaviate
    networks:
      - gremlins-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/v1/.well-known/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Redis for Celery Task Queue
  redis:
    image: redis:7.2-alpine
    container_name: gremlins-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - gremlins-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Worker for Background Tasks
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        INSTALL_TIER: ${INSTALL_TIER:-standard}
    container_name: gremlins-celery-worker
    command: celery -A app.core.celery_app worker --loglevel=info --concurrency=2
    environment:
      # Same environment as main app
      - DATABASE_URL=sqlite:///app/data/DRYAD.AI.db
      - WEAVIATE_URL=http://weaviate:8081
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - LLM_PROVIDER=${LLM_PROVIDER:-mock}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      redis:
        condition: service_healthy
      weaviate:
        condition: service_healthy
    networks:
      - gremlins-network
    restart: unless-stopped

  # Ollama Local LLM Server (Optional)
  ollama:
    image: ollama/ollama:latest
    container_name: gremlins-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - gremlins-network
    restart: unless-stopped
    profiles:
      - local-llm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Nginx Reverse Proxy (Production)
  nginx:
    image: nginx:alpine
    container_name: gremlins-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - gremlins-api
    networks:
      - gremlins-network
    restart: unless-stopped
    profiles:
      - production

  # Monitoring with Prometheus (Optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: gremlins-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - gremlins-network
    restart: unless-stopped
    profiles:
      - monitoring

  # Grafana Dashboard (Optional)
  grafana:
    image: grafana/grafana:latest
    container_name: gremlins-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    networks:
      - gremlins-network
    restart: unless-stopped
    profiles:
      - monitoring

# Networks
networks:
  gremlins-network:
    driver: bridge

# Volumes
volumes:
  weaviate_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
